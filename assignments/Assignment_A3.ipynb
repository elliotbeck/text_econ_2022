{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment A3: Embeddings and Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covering material from notebooks 7 and 8 \n",
    "\n",
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training word2vec**\n",
    "\n",
    "In this section, we train a word2vec model using gensim. We train the model on text8 (which consists of the first 90M characters of a Wikipedia dump from 2006 and is considered one of the benchmarks for evaluating language models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 1701,\n",
       " 'record_format': 'list of str (tokens)',\n",
       " 'file_size': 33182058,\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       " 'license': 'not found',\n",
       " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       " 'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       " 'file_name': 'text8.gz',\n",
       " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "api.info(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
     ]
    }
   ],
   "source": [
    "dataset = api.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "##TODO train a word2vec model on this dataset, only consider words which appear at least 10 times in the corpus\n",
    "model = Word2Vec(sentences=dataset, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Similarities**\n",
    "\n",
    "gensim models provide almost all the utility you might want to wish for to perform standard word similarity tasks. They are available in the .wv (wordvectors) attribute of the model, more details could be found [here](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('prince', 0.7549830079078674), ('queen', 0.713581919670105), ('emperor', 0.7002630829811096), ('throne', 0.6998481154441833), ('vii', 0.6928168535232544), ('kings', 0.6912181973457336), ('regent', 0.6725978255271912), ('sigismund', 0.6714800596237183), ('aragon', 0.6675142645835876), ('elector', 0.6599542498588562)]\n"
     ]
    }
   ],
   "source": [
    "##TODO find the closest words to king\n",
    "sims = model.wv.most_similar('king', topn=10)  # get other similar words\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "King is to man as woman is to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('queen', 0.6591334342956543)\n",
      "('queen', 0.6591334342956543)\n"
     ]
    }
   ],
   "source": [
    "##TODO find the closest word for the vector \"woman\" + \"king\" - \"man\"\n",
    "print(model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])[0])\n",
    "\n",
    "# what's going on under the hood?\n",
    "vec_king, vec_man, vec_woman = model.wv.get_vector(\"king\", norm=True), model.wv.get_vector(\"man\", norm=True), model.wv.get_vector(\"woman\", norm=True)\n",
    "result = model.wv.similar_by_vector(vec_king - vec_man + vec_woman)[1]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Word Similarities** \n",
    "\n",
    "One common way to evaluate word2vec models are word analogy tasks. Let's check how good our model is on one of those. We consider the [WordSim353](http://alfonseca.org/eng/research/wordsim353.html) benchmark, the task is to determine how similar two words are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-21 11:11:55--  http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
      "Resolving alfonseca.org (alfonseca.org)... 162.215.249.67\n",
      "Connecting to alfonseca.org (alfonseca.org)|162.215.249.67|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5460 (5.3K) [application/x-gzip]\n",
      "Saving to: 'ws353simrel.tar.gz'\n",
      "\n",
      "ws353simrel.tar.gz  100%[===================>]   5.33K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-11-21 11:11:56 (82.7 MB/s) - 'ws353simrel.tar.gz' saved [5460/5460]\n",
      "\n",
      "[('tiger', 'cat'), ('tiger', 'tiger'), ('plane', 'car')] [7.35, 10.0, 5.77]\n"
     ]
    }
   ],
   "source": [
    "!wget http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
    "!tar xf ws353simrel.tar.gz\n",
    "\n",
    "path = \"wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n",
    "\n",
    "def load_data(path):\n",
    "    X, y = [], []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            X.append((line[0], line[1])) # each entry in x contains two words, e.g. X[0] = (tiger, cat)\n",
    "            y.append(float(line[-1])) # each entry in y is the annotation how similar two words are, e.g. Y[0] = 7.35\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data(path)\n",
    "print (X[:3], y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59547836\n",
      "1.0\n",
      "0.43516302\n",
      "[0.59547836, 1.0, 0.43516302, 0.5323238, 0.74368805, 0.5204391, 0.68291867, 0.71805686, 0.51681644, 0.43424064, 0.6265508, 0.5278405, 0.36214787, 0.6459682, 0.713582, 0.2549085, 0.5573806, 0.1103385, 0.81987226, 0.7906139, 0.71744037, 0, 0.7581198, 0.6396023, 0.7743103, 0.7803402, 0.60831726, 0.39107767, 0.75948596, 0.36391062, 0.75969815, 0, 0.71910495, 0.7711973, 0.5829115, 0.69855523, 0.3797861, 0.5059833, 0.046145216, 0.38520706, 0.40653822, 0.6798787, 0.37402877, 0.1627388, 0.3341751, 0.22194439, -0.009029146, 0.26948124, 0.7713224, 0.66677904, 0.57125586, 0.577476, 0.7543255, 0.62265646, 0.36359268, 0.41836524, 0, 0.11971237, -0.11856868, -0.04326883, 0.4479499, 0.57966703, 0.811289, 0.7102834, 0, 0.3007991, 0.42208344, -0.029927105, 0, 0.15269986, 0.3453254, 0.15259264, 0.58035094, 0.19801103, 0.7063954, 0.23811662, 0.34194934, 0.5081974, 0.75541615, 0.35168144, 0.5062486, 0.15133363, 0.37584507, 0.6490791, 0.4026931, 0.69814104, 0.69137526, 0.30182022, 0.5396628, 0.8316394, 0.7303989, 0.09444672, 0.67475116, 0.24730027, 0, 0.65096134, 0.5660305, 0.72475356, 0.5929663, 0.62677056, 0.20453587, 0.022059161, 0.4627763, 0.02912626, 0.44777796, 0.16391918, 0.5229542, 0.34637737, 0.48682338, 0.33589542, 0.09097726, 0.15590957, 0.3337682, 0.19807824, 0.11047202, 0.46392715, 0.119136095, 0.37865546, 0.49989563, 0.12147166, 0.43515137, 0.1763817, 0.26833224, 0.2430644, 0.29504654, 0.2757706, 0.264228, 0.20724349, 0.18040112, 0.4846658, 0.22462833, 0.22965208, 0.40294504, 0.11017353, 0.13953121, 0.13790852, 0.21540806, -0.043217715, 0.47569752, 0.05710811, 0.3346311, 0.06447951, 0.07455905, 0.15473951, 0.27243856, 0.26169366, 0.32410085, 0.2067101, 0.36184323, 0.061903533, 0.06472351, 0.014886811, 0.40611106, 0.173702, 0.43522024, 0.24480784, 0.22848466, 0.2774491, 0.2820245, 0.2144626, 0.093681924, 0, 0.18244876, 0.101369575, 0.4026722, -0.07883818, -0.30570045, 0.34949392, 0.23526472, 0.043626487, -0.12284226, 0.09353708, -0.024298854, 0.071960896, 0.11671546, 0.0642601, 0.28066856, 0, 0.16300106, 0.17360488, -0.075344995, -0.06255467, 0.36536756, 0.07295367, 0.14210904, 0.11832075, 0.08955391, 0.051427007, 0.29851484, 0.053336196, 0, 0.412656, -0.08339949, -0.032437544, 0.36842775, 0.22629406, 0.45533645, -0.09437832, 0.16897756, 0.0917671, 0.114158906, -0.003952574, -0.0033814833]\n"
     ]
    }
   ],
   "source": [
    "##TODO compute how similar the pairs in the WordSim353 are according to our model\n",
    "##TODO if  aword is not present in our model, we assign similarity 0 for the respective text pair\n",
    "results = []\n",
    "for i in range(len(X)):\n",
    "    try:\n",
    "        vec1, vec2 = model.wv.get_vector(X[i][0], norm=True), model.wv.get_vector(X[i][1], norm=True)\n",
    "        results.append(model.wv.similarity(X[i][0], X[i][1]))\n",
    "    except:\n",
    "        results.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.6514524845694514, pvalue=6.697848116658943e-26)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "##TODO compute spearman's rank correlation between our prediction and the human annotations\n",
    "spearmanr(y, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.6445293311100014, pvalue=3.203028082291722e-25)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "en = spacy.load('en_core_web_md')\n",
    "\n",
    "##TODO compute word similarities in the WordSim353 dataset using spaCy word embeddings\n",
    "results_spacy = []\n",
    "for i in range(len(X)):\n",
    "    try:\n",
    "        results_spacy.append(en(X[i][0]).similarity(en(X[i][1])))\n",
    "    except:\n",
    "        results_spacy.append(0)\n",
    "##TODO compute spearman's rank correlation between these similarities and the human annotations\n",
    "spearmanr(y, results_spacy)\n",
    "# Don't worry if results are not too convincing for this experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "In this task, we evaluate different document embeddings on the English version of the [STS Benchmark](https://arxiv.org/pdf/1708.00055.pdf). The task is to determine how semantically similar two texts are and is a popular dataset to evaluate document embeddings, i.e. we want embeddings of two semantically similar documents to be similar as well. We provide a wordcounts baseline for this task and ask you to compute and evaluate embeddings for a selected sample of document embedding techniques.\n",
    "\n",
    "To evaluate, we follow [(Reimers and Gurevych, 2019)](https://arxiv.org/pdf/1908.10084.pdf) and compute the Spearmanâ€™s rank correlation between the cosine-similarity of thesentence embeddings and the gold labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-21 11:44:12--  http://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.eval.v1.1.zip\n",
      "Resolving alt.qcri.org (alt.qcri.org)... 80.76.166.234\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.234|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.eval.v1.1.zip [following]\n",
      "--2022-11-21 11:44:12--  https://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.eval.v1.1.zip\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.234|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 87902 (86K) [application/zip]\n",
      "Saving to: 'sts2017.eval.v1.1.zip'\n",
      "\n",
      "sts2017.eval.v1.1.z 100%[===================>]  85.84K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-11-21 11:44:13 (647 KB/s) - 'sts2017.eval.v1.1.zip' saved [87902/87902]\n",
      "\n",
      "URL transformed to HTTPS due to an HSTS policy\n",
      "--2022-11-21 11:44:13--  https://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.gs.zip\n",
      "Resolving alt.qcri.org (alt.qcri.org)... 80.76.166.234\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.234|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3138 (3.1K) [application/zip]\n",
      "Saving to: 'sts2017.gs.zip'\n",
      "\n",
      "sts2017.gs.zip      100%[===================>]   3.06K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-11-21 11:44:14 (1.46 GB/s) - 'sts2017.gs.zip' saved [3138/3138]\n",
      "\n",
      "Archive:  sts2017.eval.v1.1.zip\n",
      "  inflating: STS2017.eval.v1.1/LICENSE.txt  \n",
      "  inflating: STS2017.eval.v1.1/README.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track1.ar-ar.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track2.ar-en.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track3.es-es.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track4a.es-en.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track4b.es-en.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track5.en-en.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track6.tr-en.txt  \n",
      "Archive:  sts2017.gs.zip\n",
      "   creating: STS2017.gs/\n",
      "  inflating: STS2017.gs/STS.gs.track1.ar-ar.txt  \n",
      "  inflating: STS2017.gs/STS.gs.track2.ar-en.txt  \n",
      "  inflating: STS2017.gs/STS.gs.track3.es-es.txt  \n",
      "  inflating: STS2017.gs/STS.gs.track4a.es-en.txt  \n",
      "  inflating: STS2017.gs/STS.gs.track4b.es-en.txt  \n",
      "  inflating: STS2017.gs/STS.gs.track5.en-en.txt  \n",
      "  inflating: STS2017.gs/STS.gs.track6.tr-en.txt  \n"
     ]
    }
   ],
   "source": [
    "# obtain the data\n",
    "!wget http://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.eval.v1.1.zip\n",
    "!wget http://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.gs.zip\n",
    "\n",
    "!unzip sts2017.eval.v1.1.zip \n",
    "!unzip sts2017.gs.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person is on a baseball team.',\n",
       " 'A person is playing basketball on a team.',\n",
       " 2.4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "def load_STS_data():\n",
    "    with open(\"STS2017.gs/STS.gs.track5.en-en.txt\") as f:\n",
    "        labels = [float(line.strip()) for line in f]\n",
    "    \n",
    "    text_a, text_b = [], []\n",
    "    with open(\"STS2017.eval.v1.1/STS.input.track5.en-en.txt\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            text_a.append(line[0])\n",
    "            text_b.append(line[1])\n",
    "    return text_a, text_b, labels\n",
    "\n",
    "text_a, text_b, labels = load_STS_data()\n",
    "text_a[0], text_b[0], labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utils\n",
    "from scipy.stats import spearmanr\n",
    "def evaluate(predictions, labels):\n",
    "    print (\"spearman's rank correlation\", spearmanr(predictions, labels)[0])\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman's rank correlation 0.6998056665685976\n"
     ]
    }
   ],
   "source": [
    "# Wordcounts baseline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "vec.fit(text_a + text_b)\n",
    "\n",
    "# encode documents\n",
    "text_a_encoded = np.array(vec.transform(text_a).todense())\n",
    "text_b_encoded = np.array(vec.transform(text_b).todense())\n",
    "\n",
    "# predict cosine similarities\n",
    "predictions = [cosine_similarity(a,b) for a,b in zip(text_a_encoded, text_b_encoded)]\n",
    "\n",
    "# evaluate\n",
    "evaluate(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman's rank correlation 0.0752375525451465\n"
     ]
    }
   ],
   "source": [
    "##TODO train Doc2Vec on the texts in the dataset\n",
    "##TODO derive the word vectors for each text in the dataset\n",
    "##TODO compute cosine similarity between the text pairs and evaluate spearman's rank correlation\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "text_a_raw = [TaggedDocument(doc, [i]) for i, doc in enumerate(text_a)]\n",
    "text_b_raw = [TaggedDocument(doc, [i]) for i, doc in enumerate(text_b)]\n",
    "text = text_a_raw + text_b_raw\n",
    "model = Doc2Vec(text)\n",
    "text_a_encoded = np.array([model.infer_vector(doc.words.split()) for doc in text_a_raw])\n",
    "text_b_encoded = np.array([model.infer_vector(doc.words.split()) for doc in text_b_raw])\n",
    "\n",
    "# # predict cosine similarities\n",
    "predictions = [cosine_similarity(a,b) for a,b in zip(text_a_encoded, text_b_encoded)]\n",
    "\n",
    "# evaluate\n",
    "evaluate(predictions, labels)\n",
    "\n",
    "## Don't worry if results are not satisfactory using Doc2Vec (the dataset is too small to train good embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman's rank correlation 0.4434417833016249\n"
     ]
    }
   ],
   "source": [
    "##TODO do the same with embeddings provided by spaCy\n",
    "##TODO compute word similarities in the WordSim353 dataset using spaCy word embeddings\n",
    "text_a_encoded = np.array([en(text).vector for text in text_a])\n",
    "text_b_encoded = np.array([en(text).vector for text in text_b])\n",
    "\n",
    "# # predict cosine similarities\n",
    "predictions = [cosine_similarity(a,b) for a,b in zip(text_a_encoded, text_b_encoded)]\n",
    "\n",
    "# evaluate\n",
    "evaluate(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 14:04:15.796839: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman's rank correlation 0.0038053103255100895\n"
     ]
    }
   ],
   "source": [
    "##TODO do the same with universal sentence embeddings\n",
    "# import necessary libraries\n",
    "import tensorflow_hub as hub\n",
    "  \n",
    "# Load pre-trained universal sentence encoder model\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "  \n",
    "# encode documents\n",
    "text_a_encoded = np.array(embed(text_a))\n",
    "text_b_encoded = np.array(embed(text_a))\n",
    "\n",
    "# # predict cosine similarities\n",
    "predictions = [cosine_similarity(a,b) for a,b in zip(text_a_encoded, text_b_encoded)]\n",
    "\n",
    "# evaluate\n",
    "evaluate(predictions, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "Use your favorite document embeddings method to compute embeddings for a dataset you are interested in. Think of a method and provide some data visualization statistics (one method would be the path we have chosen in the notebook, i.e. cluster the embeddings with k-means and visualize low-dimensional representations of the document embeddings obtained by PCA). \n",
    "\n",
    "This task is very open and there is no right or wrong; If you want to use document embeddings in your course project, this is a chance to play around with them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "883fe47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import tweepy\n",
    "import spacy\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "\n",
    "#Â load environment variables\n",
    "load_dotenv()\n",
    "consumer_key = os.environ[\"API_KEY\"]\n",
    "consumer_secret = os.environ[\"API_KEY_SECRET\"]\n",
    "access_token = os.environ[\"ACCESS_TOKEN\"]\n",
    "access_token_secret = os.environ[\"ACCESS_TOKEN_SECRET\"]\n",
    "\n",
    "# authenticate\n",
    "auth = tweepy.OAuth1UserHandler(\n",
    "  consumer_key, \n",
    "  consumer_secret, \n",
    "  access_token, \n",
    "  access_token_secret\n",
    ")\n",
    "\n",
    "# connect to twitter\n",
    "api = tweepy.API(auth)\n",
    "api.verify_credentials()\n",
    "\n",
    "# set query topic\n",
    "queryTopic = 'inflation -filter:retweets'\n",
    "\n",
    "# get the pages\n",
    "extracted_pages = []\n",
    "for page in tweepy.Cursor(api.search_tweets, \n",
    "                            queryTopic, \n",
    "                            lang=\"de\",\n",
    "                            count=100).pages(10):\n",
    "    extracted_pages.append(page)\n",
    "    \n",
    "# get the tweets\n",
    "tweets = []\n",
    "for page in extracted_pages:\n",
    "    tweets += page\n",
    "\n",
    "# convert data to pandas df\n",
    "json_data = [r._json for r in tweets]\n",
    "df = pd.json_normalize(json_data)\n",
    "\n",
    "# select subset of columns\n",
    "df = df[[\"created_at\", \"text\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f825ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# get stopwords\n",
    "def get_stopwords():\n",
    "    \"Return a set of stopwords read in from a file.\"\n",
    "    with open(\"stop_words_german.txt\") as f:\n",
    "        stopwords = []\n",
    "        for line in f:\n",
    "            stopwords.append(line.strip(\"\\n\"))\n",
    "    # Convert to set for performance\n",
    "    stopwords_set = set(stopwords)\n",
    "    return stopwords_set\n",
    "stopwords = get_stopwords()\n",
    "\n",
    "# define leammatization function\n",
    "def lemmatize_pipe(doc):\n",
    "    lemma_list = [str(tok.lemma_).lower() for tok in doc\n",
    "                  if tok.is_alpha and tok.text.lower() not in stopwords] \n",
    "    return lemma_list\n",
    "\n",
    "# define lemmatization pipe\n",
    "def preprocess_pipe(texts):\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=20):\n",
    "        preproc_pipe.append(lemmatize_pipe(doc))\n",
    "    return preproc_pipe\n",
    "\n",
    "# cleaned text\n",
    "df['preproc_pipe'] = preprocess_pipe(df['text'])\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f860e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â get embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a0e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 1 3 0 3 2 4 1]\n",
      "Cluster  1\n",
      "['@ZStadtfux Auch wenn man Unsinn wiederholt wird es nicht richtig. DieðŸ‡¨ðŸ‡­ist heute je nach Studie das beste, reichsteâ€¦ https://t.co/8oYfuexgGS']\n",
      "\n",
      "Cluster  2\n",
      "['@DrieElmann Ich dachte, wir haetten Inflation?\\nansparen von mehr als11kðŸ˜‡ðŸ˜œ', '@DannyLevievFam Nur Brot wegen inflation']\n",
      "\n",
      "Cluster  3\n",
      "['@eulebln @rbb24 @sophiamersmann @Hadmut @_donalphonso @RolandTichy @reitschuster @_richtig_falsch @destatis Wir habâ€¦ https://t.co/Vu9Aqa5GlU']\n",
      "\n",
      "Cluster  4\n",
      "['@pschnek @Clemanns1984 @volkspartei Durch die Merit Order haben wir zur Zeit eine kÃ¼nstlich aufgeblasene Inflation,â€¦ https://t.co/0Z5bTjzmdH', 'â€žDie Angst vor Ãœberfremdung, Kulturverlust oder Inflation ist verÃ¤chtlich, die Angst vor dem Weltuntergang ist gut.â€¦ https://t.co/LHyuLpLMpq', '@HerrNMaus Sagen wir es Mal so in 10 Jahren Hoffen wir dass die Inflation sich bisschen gelegt hat denn dann brauchâ€¦ https://t.co/lpP2S6JwL5', '@ChrSchumi Das Problem es ist Krieg, Inflation  Pandemie, AufstÃ¤nde Iran Energiekrise etc. und natÃ¼rlich die Fussbaâ€¦ https://t.co/qKmplDsd5T', '@xaendbomb Der Gaspreisdeckel senkt die Inflation und das fÃ¼r weniger Steuergeld, als die Regierung bisher in Einmaâ€¦ https://t.co/BDEALtg3Tj']\n",
      "\n",
      "Cluster  5\n",
      "['Angestellte in tarifgebundenen Pflegeeinrichtungen verdienen im Schnitt rund 20,37 Euro pro Stunde. Ãœber groÃŸe Lohnâ€¦ https://t.co/9lBKzL5FHa']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Perform kmean clustering\n",
    "num_clusters = 5\n",
    "clustering_model = KMeans(n_clusters=num_clusters)\n",
    "clustering_model.fit(embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(df.loc[sentence_id, 'text'])\n",
    "\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(\"Cluster \", i+1)\n",
    "    print(cluster)\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e9e654e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Cluster=3<br>PCA component 1=%{x}<br>PCA component 2=%{y}<extra></extra>",
         "legendgroup": "3",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "3",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0.5340244770050049,
          -0.3243490755558014,
          -3.546292543411255,
          2.165541410446167,
          1.2251924276351929
         ],
         "xaxis": "x",
         "y": [
          -1.8346043825149536,
          0.5056266188621521,
          2.4729347229003906,
          2.1966397762298584,
          -1.4773304462432861
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Cluster=1<br>PCA component 1=%{x}<br>PCA component 2=%{y}<extra></extra>",
         "legendgroup": "1",
         "marker": {
          "color": "#EF553B",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "1",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          -5.736063480377197,
          -3.112683057785034
         ],
         "xaxis": "x",
         "y": [
          -1.4147882461547852,
          -3.6974849700927734
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Cluster=0<br>PCA component 1=%{x}<br>PCA component 2=%{y}<extra></extra>",
         "legendgroup": "0",
         "marker": {
          "color": "#00cc96",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0.8639668226242065
         ],
         "xaxis": "x",
         "y": [
          6.860229969024658
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Cluster=2<br>PCA component 1=%{x}<br>PCA component 2=%{y}<extra></extra>",
         "legendgroup": "2",
         "marker": {
          "color": "#ab63fa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "2",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          8.140976905822754
         ],
         "xaxis": "x",
         "y": [
          -2.316331624984741
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Cluster=4<br>PCA component 1=%{x}<br>PCA component 2=%{y}<extra></extra>",
         "legendgroup": "4",
         "marker": {
          "color": "#FFA15A",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "4",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          -0.21031202375888824
         ],
         "xaxis": "x",
         "y": [
          -1.294891119003296
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Cluster"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "PCA component 1"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "PCA component 2"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "fig = px.scatter(components, \n",
    "                 color=cluster_assignment.astype(str), \n",
    "                 labels={\n",
    "                     \"0\": \"PCA component 1\",\n",
    "                     \"1\": \"PCA component 2\",\n",
    "                     \"color\": \"Cluster\"\n",
    "                 },\n",
    "                 x=0, \n",
    "                 y=1)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88262</th>\n",
       "      <td>world</td>\n",
       "      <td>On litter-strewn street, Palestinians mourn</td>\n",
       "      <td>After 36 years in which he influenced and domi...</td>\n",
       "      <td>On litter-strewn street, Palestinians mourn Af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74285</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>Wireless network links researchers in panda pr...</td>\n",
       "      <td>Giant pandas might prefer bamboo to laptops, b...</td>\n",
       "      <td>Wireless network links researchers in panda pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26563</th>\n",
       "      <td>sport</td>\n",
       "      <td>NFL Game Summary - Green Bay at Carolina</td>\n",
       "      <td>Charlotte, NC (Sports Network) - Ahman Green s...</td>\n",
       "      <td>NFL Game Summary - Green Bay at Carolina Charl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27726</th>\n",
       "      <td>business</td>\n",
       "      <td>Citigroup apology on bond deals</td>\n",
       "      <td>LONDON Citigroup told employees on Tuesday tha...</td>\n",
       "      <td>Citigroup apology on bond deals LONDON Citigro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12072</th>\n",
       "      <td>sport</td>\n",
       "      <td>Hamm #39;s legacy should be for Olympic ideal</td>\n",
       "      <td>Even though he has fled the perceived hostile ...</td>\n",
       "      <td>Hamm #39;s legacy should be for Olympic ideal ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                              title  \\\n",
       "88262     world        On litter-strewn street, Palestinians mourn   \n",
       "74285  sci/tech  Wireless network links researchers in panda pr...   \n",
       "26563     sport           NFL Game Summary - Green Bay at Carolina   \n",
       "27726  business                    Citigroup apology on bond deals   \n",
       "12072     sport      Hamm #39;s legacy should be for Olympic ideal   \n",
       "\n",
       "                                                    lead  \\\n",
       "88262  After 36 years in which he influenced and domi...   \n",
       "74285  Giant pandas might prefer bamboo to laptops, b...   \n",
       "26563  Charlotte, NC (Sports Network) - Ahman Green s...   \n",
       "27726  LONDON Citigroup told employees on Tuesday tha...   \n",
       "12072  Even though he has fled the perceived hostile ...   \n",
       "\n",
       "                                                    text  \n",
       "88262  On litter-strewn street, Palestinians mourn Af...  \n",
       "74285  Wireless network links researchers in panda pr...  \n",
       "26563  NFL Game Summary - Green Bay at Carolina Charl...  \n",
       "27726  Citigroup apology on bond deals LONDON Citigro...  \n",
       "12072  Hamm #39;s legacy should be for Olympic ideal ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df = df.sample(n=10000) # # only use 10K datapoints\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "#TODO preprocess the corpus using spacy or load the pre-processed corpus\n",
    "docs = nlp.pipe(df[\"lead\"]) \n",
    "df[\"text_clean\"] = [[chunk.text.lower() for chunk in doc if not \n",
    "                      (chunk.is_punct or chunk.is_stop)] for doc in docs]\n",
    "df[\"text_clean\"] = df[\"text_clean\"].str.join(\" \")\n",
    "\n",
    "# use the pre-processed corpus for the rest of the exercise\n",
    "docs = list(nlp.pipe(df[\"text_clean\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('official', 'say'), 131),\n",
       " (('.', 'say'), 84),\n",
       " (('company', 'say'), 72),\n",
       " (('inc', 'say'), 70),\n",
       " (('target=/stock', 'quickinfo'), 57),\n",
       " (('corp', 'say'), 48),\n",
       " (('group', 'say'), 32),\n",
       " (('research', 'say'), 29),\n",
       " (('people', 'kill'), 29),\n",
       " (('police', 'say'), 26)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_subject_verb_pairs(sent):\n",
    "    subjs = [w for w in sent if w.dep_ == \"nsubj\"]\n",
    "    pairs = [(w.lemma_.lower(), w.head.lemma_.lower()) for w in subjs]\n",
    "    return pairs\n",
    "##TODO extract the subject-verbs pairs and print the result for the first document\n",
    "pairs = []\n",
    "for doc in docs:\n",
    "    for sent in doc.sents:\n",
    "        pairs.append(extract_subject_verb_pairs(sent))\n",
    "        \n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items\n",
    "# unlist the list of lists\n",
    "flat_pairs = [item for sublist in pairs for item in sublist]\n",
    "counted_pairs = Counter(elem for elem in flat_pairs)\n",
    "counted_pairs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('people', 'kill'), 53),\n",
       " (('point', 'score'), 26),\n",
       " (('lawsuit', 'file'), 16),\n",
       " (('rate', 'raise'), 14),\n",
       " (('plan', 'announce'), 14),\n",
       " (('soldier', 'kill'), 11),\n",
       " (('39;t', 'win'), 10),\n",
       " (('game', 'win'), 10),\n",
       " (('job', 'cut'), 10),\n",
       " (('attack', 'kill'), 10)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TODO do the same for verbs-object pairs ('dobj')\n",
    "def extract_object_verb_pairs(sent):\n",
    "    subjs = [w for w in sent if w.dep_ == \"dobj\"]\n",
    "    pairs = [(w.lemma_.lower(), w.head.lemma_.lower()) for w in subjs]\n",
    "    return pairs\n",
    "##TODO extract the subject-verbs pairs and print the result for the first document\n",
    "pairs = []\n",
    "for doc in docs:\n",
    "    for sent in doc.sents:\n",
    "        pairs.append(extract_object_verb_pairs(sent))\n",
    "        \n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items\n",
    "# unlist the list of lists\n",
    "flat_pairs = [item for sublist in pairs for item in sublist]\n",
    "counted_pairs = Counter(elem for elem in flat_pairs)\n",
    "counted_pairs.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('mobile', 'phone'), 56),\n",
       " (('presidential', 'election'), 51),\n",
       " (('second', 'quarter'), 41),\n",
       " (('open', 'source'), 39),\n",
       " (('high', 'price'), 38),\n",
       " (('fourth', 'quarter'), 33),\n",
       " (('senior', 'official'), 33),\n",
       " (('quarterly', 'profit'), 32),\n",
       " (('crude', 'oil'), 30),\n",
       " (('economic', 'growth'), 29)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TODO do the same for adjectives-nouns pairs ('amod')\n",
    "def extract_adjectives_nouns_pairs(sent):\n",
    "    subjs = [w for w in sent if w.dep_ == \"amod\"]\n",
    "    pairs = [(w.lemma_.lower(), w.head.lemma_.lower()) for w in subjs]\n",
    "    return pairs\n",
    "##TODO extract the subject-verbs pairs and print the result for the first document\n",
    "pairs = []\n",
    "for doc in docs:\n",
    "    for sent in doc.sents:\n",
    "        pairs.append(extract_adjectives_nouns_pairs(sent))\n",
    "        \n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items\n",
    "# unlist the list of lists\n",
    "flat_pairs = [item for sublist in pairs for item in sublist]\n",
    "counted_pairs = Counter(elem for elem in flat_pairs)\n",
    "counted_pairs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring cross label dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('turnover', 'win'), ('server', 'win'), ('norway', 'win'), ('singh', 'win'), ('park', 'win'), ('russia', 'win'), ('hofstra', 'win'), ('bush', 'win'), ('vote', 'win'), ('spain', 'win')]\n",
      "[('opener', 'win'), ('controversy', 'win'), ('year', 'win'), ('39;t', 'win'), ('rally', 'win'), ('game', 'win'), ('course', 'win'), ('bridge', 'win'), ('korea', 'win'), ('win', 'win')]\n"
     ]
    }
   ],
   "source": [
    "##TODO extract all the subject-verbs and verbs-object pairs for the verb \"win\"\n",
    "pairs = []\n",
    "for doc in docs:\n",
    "    for sent in doc.sents:\n",
    "        pairs.append(extract_subject_verb_pairs(sent))\n",
    "        \n",
    "# unlist the list of lists\n",
    "flat_pairs = [item for sublist in pairs for item in sublist]\n",
    "subject_flat_pairs_filtered = [pair for pair in flat_pairs if pair[1] == \"win\"]\n",
    "print(subject_flat_pairs_filtered[0:10])\n",
    "\n",
    "pairs = []\n",
    "for doc in docs:\n",
    "    for sent in doc.sents:\n",
    "        pairs.append(extract_object_verb_pairs(sent))\n",
    "\n",
    "# unlist the list of lists\n",
    "flat_pairs = [item for sublist in pairs for item in sublist]\n",
    "object_flat_pairs_filtered = [pair for pair in flat_pairs if pair[1] == \"win\"]\n",
    "print(object_flat_pairs_filtered[0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Object pairs ####\n",
      "[(('point', 'score'), 26), (('game', 'win'), 9), (('pass', 'throw'), 9), (('series', 'win'), 8), (('surgery', 'undergo'), 6), (('season', 'miss'), 6), (('term', 'agree'), 6), (('touchdown', 'throw'), 6), (('par', 'shoot'), 5), (('title', 'win'), 5)]\n",
      "\n",
      "[(('rate', 'raise'), 14), (('job', 'cut'), 9), (('charge', 'settle'), 8), (('earning', 'report'), 8), (('inc', 'buy'), 8), (('corp', 'buy'), 8), (('research', 'quote'), 7), (('job', 'add'), 6), (('world', 'quickinfo'), 6), (('loss', 'report'), 6)]\n",
      "\n",
      "#### Subject pairs ####\n",
      "[(('quot', 'say'), 13), (('game', 'play'), 8), (('bond', 'hit'), 7), (('match', 'play'), 6), (('second', 'leave'), 6), (('antonio', 'spur'), 5), (('manning', 'throw'), 5), (('assist', 'lead'), 5), (('team', 'win'), 5), (('michael', 'phelp'), 5)]\n",
      "\n",
      "[(('.', 'say'), 57), (('target=/stock', 'quickinfo'), 57), (('inc', 'say'), 55), (('company', 'say'), 45), (('corp', 'say'), 31), (('research', 'say'), 25), (('official', 'say'), 22), (('profit', 'rise'), 21), (('group', 'say'), 18), (('target=/stock', 'say'), 14)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##TODO for each label create a list ranking the most common subject-verbs pairs and one for the most common verbs-object pairs\n",
    "object_pairs_by_label = []\n",
    "for label in label_map:\n",
    "    df_label = df.loc[df['label'] == label_map[label]]\n",
    "    docs = list(nlp.pipe(df_label[\"text_clean\"]))\n",
    "    ##TODO extract the subject-verbs pairs and print the result for the first document\n",
    "    pairs = []\n",
    "    for doc in docs:\n",
    "        for sent in doc.sents:\n",
    "            pairs.append(extract_object_verb_pairs(sent))\n",
    "            \n",
    "    from collections import Counter\n",
    "    counter = Counter()\n",
    "\n",
    "    # unlist the list of lists\n",
    "    flat_pairs = [item for sublist in pairs for item in sublist]\n",
    "    counted_pairs = Counter(elem for elem in flat_pairs)\n",
    "    object_pairs_by_label.append(counted_pairs)\n",
    "    \n",
    "subject_pairs_by_label = []\n",
    "for label in label_map:\n",
    "    df_label = df.loc[df['label'] == label_map[label]]\n",
    "    docs = list(nlp.pipe(df_label[\"text_clean\"]))\n",
    "    ##TODO extract the subject-verbs pairs and print the result for the first document\n",
    "    pairs = []\n",
    "    for doc in docs:\n",
    "        for sent in doc.sents:\n",
    "            pairs.append(extract_subject_verb_pairs(sent))\n",
    "            \n",
    "    from collections import Counter\n",
    "    counter = Counter()\n",
    "\n",
    "    # unlist the list of lists\n",
    "    flat_pairs = [item for sublist in pairs for item in sublist]\n",
    "    counted_pairs = Counter(elem for elem in flat_pairs)\n",
    "    subject_pairs_by_label.append(counted_pairs)\n",
    "    \n",
    "##TODO print the 10 most common pairs for each of the two lists for the labels \"sport\" and \"business\"\n",
    "print(\"#### Object pairs ####\")\n",
    "for i, sublist in enumerate(object_pairs_by_label):\n",
    "    if (i == 1) or (i == 2):\n",
    "        print(sublist.most_common(10))\n",
    "        print(\"\")\n",
    "print(\"#### Subject pairs ####\")\n",
    "for i, sublist in enumerate(subject_pairs_by_label):\n",
    "    if (i == 1) or (i == 2):\n",
    "        print(sublist.most_common(10))\n",
    "        print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
