{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment A3: Embeddings and Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covering material from notebooks 7 and 8 \n",
    "\n",
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training word2vec**\n",
    "\n",
    "In this section, we train a word2vec model using gensim. We train the model on text8 (which consists of the first 90M characters of a Wikipedia dump from 2006 and is considered one of the benchmarks for evaluating language models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 1701,\n",
       " 'record_format': 'list of str (tokens)',\n",
       " 'file_size': 33182058,\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       " 'license': 'not found',\n",
       " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       " 'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       " 'file_name': 'text8.gz',\n",
       " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "api.info(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
     ]
    }
   ],
   "source": [
    "dataset = api.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "##TODO train a word2vec model on this dataset, only consider words which appear at least 10 times in the corpus\n",
    "model = Word2Vec(sentences=dataset, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Similarities**\n",
    "\n",
    "gensim models provide almost all the utility you might want to wish for to perform standard word similarity tasks. They are available in the .wv (wordvectors) attribute of the model, more details could be found [here](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('prince', 0.7549830079078674), ('queen', 0.713581919670105), ('emperor', 0.7002630829811096), ('throne', 0.6998481154441833), ('vii', 0.6928168535232544), ('kings', 0.6912181973457336), ('regent', 0.6725978255271912), ('sigismund', 0.6714800596237183), ('aragon', 0.6675142645835876), ('elector', 0.6599542498588562)]\n"
     ]
    }
   ],
   "source": [
    "##TODO find the closest words to king\n",
    "sims = model.wv.most_similar('king', topn=10)  # get other similar words\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "King is to man as woman is to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('queen', 0.6591334342956543)\n",
      "('queen', 0.6591334342956543)\n"
     ]
    }
   ],
   "source": [
    "##TODO find the closest word for the vector \"woman\" + \"king\" - \"man\"\n",
    "print(model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])[0])\n",
    "\n",
    "# what's going on under the hood?\n",
    "vec_king, vec_man, vec_woman = model.wv.get_vector(\"king\", norm=True), model.wv.get_vector(\"man\", norm=True), model.wv.get_vector(\"woman\", norm=True)\n",
    "result = model.wv.similar_by_vector(vec_king - vec_man + vec_woman)[1]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Word Similarities** \n",
    "\n",
    "One common way to evaluate word2vec models are word analogy tasks. Let's check how good our model is on one of those. We consider the [WordSim353](http://alfonseca.org/eng/research/wordsim353.html) benchmark, the task is to determine how similar two words are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-21 11:11:55--  http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
      "Resolving alfonseca.org (alfonseca.org)... 162.215.249.67\n",
      "Connecting to alfonseca.org (alfonseca.org)|162.215.249.67|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5460 (5.3K) [application/x-gzip]\n",
      "Saving to: 'ws353simrel.tar.gz'\n",
      "\n",
      "ws353simrel.tar.gz  100%[===================>]   5.33K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-11-21 11:11:56 (82.7 MB/s) - 'ws353simrel.tar.gz' saved [5460/5460]\n",
      "\n",
      "[('tiger', 'cat'), ('tiger', 'tiger'), ('plane', 'car')] [7.35, 10.0, 5.77]\n"
     ]
    }
   ],
   "source": [
    "!wget http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
    "!tar xf ws353simrel.tar.gz\n",
    "\n",
    "path = \"wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n",
    "\n",
    "def load_data(path):\n",
    "    X, y = [], []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            X.append((line[0], line[1])) # each entry in x contains two words, e.g. X[0] = (tiger, cat)\n",
    "            y.append(float(line[-1])) # each entry in y is the annotation how similar two words are, e.g. Y[0] = 7.35\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data(path)\n",
    "print (X[:3], y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59547836\n",
      "1.0\n",
      "0.43516302\n",
      "[0.59547836, 1.0, 0.43516302, 0.5323238, 0.74368805, 0.5204391, 0.68291867, 0.71805686, 0.51681644, 0.43424064, 0.6265508, 0.5278405, 0.36214787, 0.6459682, 0.713582, 0.2549085, 0.5573806, 0.1103385, 0.81987226, 0.7906139, 0.71744037, 0, 0.7581198, 0.6396023, 0.7743103, 0.7803402, 0.60831726, 0.39107767, 0.75948596, 0.36391062, 0.75969815, 0, 0.71910495, 0.7711973, 0.5829115, 0.69855523, 0.3797861, 0.5059833, 0.046145216, 0.38520706, 0.40653822, 0.6798787, 0.37402877, 0.1627388, 0.3341751, 0.22194439, -0.009029146, 0.26948124, 0.7713224, 0.66677904, 0.57125586, 0.577476, 0.7543255, 0.62265646, 0.36359268, 0.41836524, 0, 0.11971237, -0.11856868, -0.04326883, 0.4479499, 0.57966703, 0.811289, 0.7102834, 0, 0.3007991, 0.42208344, -0.029927105, 0, 0.15269986, 0.3453254, 0.15259264, 0.58035094, 0.19801103, 0.7063954, 0.23811662, 0.34194934, 0.5081974, 0.75541615, 0.35168144, 0.5062486, 0.15133363, 0.37584507, 0.6490791, 0.4026931, 0.69814104, 0.69137526, 0.30182022, 0.5396628, 0.8316394, 0.7303989, 0.09444672, 0.67475116, 0.24730027, 0, 0.65096134, 0.5660305, 0.72475356, 0.5929663, 0.62677056, 0.20453587, 0.022059161, 0.4627763, 0.02912626, 0.44777796, 0.16391918, 0.5229542, 0.34637737, 0.48682338, 0.33589542, 0.09097726, 0.15590957, 0.3337682, 0.19807824, 0.11047202, 0.46392715, 0.119136095, 0.37865546, 0.49989563, 0.12147166, 0.43515137, 0.1763817, 0.26833224, 0.2430644, 0.29504654, 0.2757706, 0.264228, 0.20724349, 0.18040112, 0.4846658, 0.22462833, 0.22965208, 0.40294504, 0.11017353, 0.13953121, 0.13790852, 0.21540806, -0.043217715, 0.47569752, 0.05710811, 0.3346311, 0.06447951, 0.07455905, 0.15473951, 0.27243856, 0.26169366, 0.32410085, 0.2067101, 0.36184323, 0.061903533, 0.06472351, 0.014886811, 0.40611106, 0.173702, 0.43522024, 0.24480784, 0.22848466, 0.2774491, 0.2820245, 0.2144626, 0.093681924, 0, 0.18244876, 0.101369575, 0.4026722, -0.07883818, -0.30570045, 0.34949392, 0.23526472, 0.043626487, -0.12284226, 0.09353708, -0.024298854, 0.071960896, 0.11671546, 0.0642601, 0.28066856, 0, 0.16300106, 0.17360488, -0.075344995, -0.06255467, 0.36536756, 0.07295367, 0.14210904, 0.11832075, 0.08955391, 0.051427007, 0.29851484, 0.053336196, 0, 0.412656, -0.08339949, -0.032437544, 0.36842775, 0.22629406, 0.45533645, -0.09437832, 0.16897756, 0.0917671, 0.114158906, -0.003952574, -0.0033814833]\n"
     ]
    }
   ],
   "source": [
    "##TODO compute how similar the pairs in the WordSim353 are according to our model\n",
    "##TODO if  aword is not present in our model, we assign similarity 0 for the respective text pair\n",
    "results = []\n",
    "for i in range(len(X)):\n",
    "    try:\n",
    "        vec1, vec2 = model.wv.get_vector(X[i][0], norm=True), model.wv.get_vector(X[i][1], norm=True)\n",
    "        results.append(model.wv.similarity(X[i][0], X[i][1]))\n",
    "    except:\n",
    "        results.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.6514524845694514, pvalue=6.697848116658943e-26)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "##TODO compute spearman's rank correlation between our prediction and the human annotations\n",
    "spearmanr(y, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.6445293311100014, pvalue=3.203028082291722e-25)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "en = spacy.load('en_core_web_md')\n",
    "\n",
    "##TODO compute word similarities in the WordSim353 dataset using spaCy word embeddings\n",
    "results_spacy = []\n",
    "for i in range(len(X)):\n",
    "    try:\n",
    "        results_spacy.append(en(X[i][0]).similarity(en(X[i][1])))\n",
    "    except:\n",
    "        results_spacy.append(0)\n",
    "##TODO compute spearman's rank correlation between these similarities and the human annotations\n",
    "spearmanr(y, results_spacy)\n",
    "# Don't worry if results are not too convincing for this experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "In this task, we evaluate different document embeddings on the English version of the [STS Benchmark](https://arxiv.org/pdf/1708.00055.pdf). The task is to determine how semantically similar two texts are and is a popular dataset to evaluate document embeddings, i.e. we want embeddings of two semantically similar documents to be similar as well. We provide a wordcounts baseline for this task and ask you to compute and evaluate embeddings for a selected sample of document embedding techniques.\n",
    "\n",
    "To evaluate, we follow [(Reimers and Gurevych, 2019)](https://arxiv.org/pdf/1908.10084.pdf) and compute the Spearman’s rank correlation between the cosine-similarity of thesentence embeddings and the gold labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-21 11:44:12--  http://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.eval.v1.1.zip\n",
      "Resolving alt.qcri.org (alt.qcri.org)... 80.76.166.234\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.234|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.eval.v1.1.zip [following]\n",
      "--2022-11-21 11:44:12--  https://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.eval.v1.1.zip\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.234|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 87902 (86K) [application/zip]\n",
      "Saving to: 'sts2017.eval.v1.1.zip'\n",
      "\n",
      "sts2017.eval.v1.1.z 100%[===================>]  85.84K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-11-21 11:44:13 (647 KB/s) - 'sts2017.eval.v1.1.zip' saved [87902/87902]\n",
      "\n",
      "URL transformed to HTTPS due to an HSTS policy\n",
      "--2022-11-21 11:44:13--  https://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.gs.zip\n",
      "Resolving alt.qcri.org (alt.qcri.org)... 80.76.166.234\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.234|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3138 (3.1K) [application/zip]\n",
      "Saving to: 'sts2017.gs.zip'\n",
      "\n",
      "sts2017.gs.zip      100%[===================>]   3.06K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-11-21 11:44:14 (1.46 GB/s) - 'sts2017.gs.zip' saved [3138/3138]\n",
      "\n",
      "Archive:  sts2017.eval.v1.1.zip\n",
      "  inflating: STS2017.eval.v1.1/LICENSE.txt  \n",
      "  inflating: STS2017.eval.v1.1/README.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track1.ar-ar.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track2.ar-en.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track3.es-es.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track4a.es-en.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track4b.es-en.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track5.en-en.txt  \n",
      "  inflating: STS2017.eval.v1.1/STS.input.track6.tr-en.txt  \n",
      "Archive:  sts2017.gs.zip\n",
      "   creating: STS2017.gs/\n",
      "  inflating: STS2017.gs/STS.gs.track1.ar-ar.txt  \n",
      "  inflating: STS2017.gs/STS.gs.track2.ar-en.txt  \n",
      "  inflating: STS2017.gs/STS.gs.track3.es-es.txt  \n",
      "  inflating: STS2017.gs/STS.gs.track4a.es-en.txt  \n",
      "  inflating: STS2017.gs/STS.gs.track4b.es-en.txt  \n",
      "  inflating: STS2017.gs/STS.gs.track5.en-en.txt  \n",
      "  inflating: STS2017.gs/STS.gs.track6.tr-en.txt  \n"
     ]
    }
   ],
   "source": [
    "# obtain the data\n",
    "!wget http://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.eval.v1.1.zip\n",
    "!wget http://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.gs.zip\n",
    "\n",
    "!unzip sts2017.eval.v1.1.zip \n",
    "!unzip sts2017.gs.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person is on a baseball team.',\n",
       " 'A person is playing basketball on a team.',\n",
       " 2.4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "def load_STS_data():\n",
    "    with open(\"STS2017.gs/STS.gs.track5.en-en.txt\") as f:\n",
    "        labels = [float(line.strip()) for line in f]\n",
    "    \n",
    "    text_a, text_b = [], []\n",
    "    with open(\"STS2017.eval.v1.1/STS.input.track5.en-en.txt\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            text_a.append(line[0])\n",
    "            text_b.append(line[1])\n",
    "    return text_a, text_b, labels\n",
    "\n",
    "text_a, text_b, labels = load_STS_data()\n",
    "text_a[0], text_b[0], labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utils\n",
    "from scipy.stats import spearmanr\n",
    "def evaluate(predictions, labels):\n",
    "    print (\"spearman's rank correlation\", spearmanr(predictions, labels)[0])\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman's rank correlation 0.6998056665685976\n"
     ]
    }
   ],
   "source": [
    "# Wordcounts baseline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "vec.fit(text_a + text_b)\n",
    "\n",
    "# encode documents\n",
    "text_a_encoded = np.array(vec.transform(text_a).todense())\n",
    "text_b_encoded = np.array(vec.transform(text_b).todense())\n",
    "\n",
    "# predict cosine similarities\n",
    "predictions = [cosine_similarity(a,b) for a,b in zip(text_a_encoded, text_b_encoded)]\n",
    "\n",
    "# evaluate\n",
    "evaluate(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman's rank correlation 0.0752375525451465\n"
     ]
    }
   ],
   "source": [
    "##TODO train Doc2Vec on the texts in the dataset\n",
    "##TODO derive the word vectors for each text in the dataset\n",
    "##TODO compute cosine similarity between the text pairs and evaluate spearman's rank correlation\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "text_a_raw = [TaggedDocument(doc, [i]) for i, doc in enumerate(text_a)]\n",
    "text_b_raw = [TaggedDocument(doc, [i]) for i, doc in enumerate(text_b)]\n",
    "text = text_a_raw + text_b_raw\n",
    "model = Doc2Vec(text)\n",
    "text_a_encoded = np.array([model.infer_vector(doc.words.split()) for doc in text_a_raw])\n",
    "text_b_encoded = np.array([model.infer_vector(doc.words.split()) for doc in text_b_raw])\n",
    "\n",
    "# # predict cosine similarities\n",
    "predictions = [cosine_similarity(a,b) for a,b in zip(text_a_encoded, text_b_encoded)]\n",
    "\n",
    "# evaluate\n",
    "evaluate(predictions, labels)\n",
    "\n",
    "## Don't worry if results are not satisfactory using Doc2Vec (the dataset is too small to train good embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman's rank correlation 0.4434417833016249\n"
     ]
    }
   ],
   "source": [
    "##TODO do the same with embeddings provided by spaCy\n",
    "##TODO compute word similarities in the WordSim353 dataset using spaCy word embeddings\n",
    "text_a_encoded = np.array([en(text).vector for text in text_a])\n",
    "text_b_encoded = np.array([en(text).vector for text in text_b])\n",
    "\n",
    "# # predict cosine similarities\n",
    "predictions = [cosine_similarity(a,b) for a,b in zip(text_a_encoded, text_b_encoded)]\n",
    "\n",
    "# evaluate\n",
    "evaluate(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO do the same with universal sentence embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "Use your favorite document embeddings method to compute embeddings for a dataset you are interested in. Think of a method and provide some data visualization statistics (one method would be the path we have chosen in the notebook, i.e. cluster the embeddings with k-means and visualize low-dimensional representations of the document embeddings obtained by PCA). \n",
    "\n",
    "This task is very open and there is no right or wrong; If you want to use document embeddings in your course project, this is a chance to play around with them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110581</th>\n",
       "      <td>sport</td>\n",
       "      <td>UEFA to probe Valencia-Werder Bremen incidents</td>\n",
       "      <td>UEFA will be launching disciplinary proceeding...</td>\n",
       "      <td>UEFA to probe Valencia-Werder Bremen incidents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15925</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>New iMac tries to play it cool</td>\n",
       "      <td>Hot G5 chip requires some serious effort to av...</td>\n",
       "      <td>New iMac tries to play it cool Hot G5 chip req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46720</th>\n",
       "      <td>business</td>\n",
       "      <td>Ford Reports Disappointing U.S. Sales</td>\n",
       "      <td>Ford's car and truck business sales fell nearl...</td>\n",
       "      <td>Ford Reports Disappointing U.S. Sales Ford's c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57350</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>Microsoft Upgrades Windows XP Media Center (Ne...</td>\n",
       "      <td>NewsFactor - Bill Gates is about to announce a...</td>\n",
       "      <td>Microsoft Upgrades Windows XP Media Center (Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90601</th>\n",
       "      <td>sport</td>\n",
       "      <td>Rams make statement they #39;re team to beat i...</td>\n",
       "      <td>ST. LOUIS -- When St. Louis coach Mike Martz l...</td>\n",
       "      <td>Rams make statement they #39;re team to beat i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                              title  \\\n",
       "110581     sport     UEFA to probe Valencia-Werder Bremen incidents   \n",
       "15925   sci/tech                     New iMac tries to play it cool   \n",
       "46720   business              Ford Reports Disappointing U.S. Sales   \n",
       "57350   sci/tech  Microsoft Upgrades Windows XP Media Center (Ne...   \n",
       "90601      sport  Rams make statement they #39;re team to beat i...   \n",
       "\n",
       "                                                     lead  \\\n",
       "110581  UEFA will be launching disciplinary proceeding...   \n",
       "15925   Hot G5 chip requires some serious effort to av...   \n",
       "46720   Ford's car and truck business sales fell nearl...   \n",
       "57350   NewsFactor - Bill Gates is about to announce a...   \n",
       "90601   ST. LOUIS -- When St. Louis coach Mike Martz l...   \n",
       "\n",
       "                                                     text  \n",
       "110581  UEFA to probe Valencia-Werder Bremen incidents...  \n",
       "15925   New iMac tries to play it cool Hot G5 chip req...  \n",
       "46720   Ford Reports Disappointing U.S. Sales Ford's c...  \n",
       "57350   Microsoft Upgrades Windows XP Media Center (Ne...  \n",
       "90601   Rams make statement they #39;re team to beat i...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df = df.sample(n=10000) # # only use 10K datapoints\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#TODO preprocess the corpus using spacy or load the pre-processed corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_verb_pairs(sent):\n",
    "    subjs = [w for w in sent if w.dep_ == \"nsubj\"]\n",
    "    pairs = [(w.lemma_.lower(), w.head.lemma_.lower()) for w in subjs]\n",
    "    return pairs\n",
    "##TODO extract the subject-verbs pairs and print the result for the first document\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO do the same for verbs-object pairs ('dobj')\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO do the same for adjectives-nouns pairs ('amod')\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring cross label dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO extract all the subject-verbs and verbs-object pairs for the verb \"win\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO for each label create a list ranking the most common subject-verbs pairs and one for the most common verbs-object pairs\n",
    "##TODO print the 10 most common pairs for each of the two lists for the labels \"sport\" and \"business\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7847a2e259535c6ee3594af5f07a48848165b814e08df174595330ed14ca1dac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
